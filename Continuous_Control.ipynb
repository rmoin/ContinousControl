{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will make use of the Unity ML-Agents environment.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages. Please make sure to install [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Don't need to run this cell. Check out the dependencies in README file\n",
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't need to run this cell. This is only required during the training in Udacity workspaces\n",
    "#from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Actor, Critic\n",
    "from utils import sample, update_targets, OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "## For Experience Replay\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128         # minibatch size\n",
    "\n",
    "## For Fixed-Q Target\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "UPDATE_EVERY = 20       # how often to update the network. \n",
    "UPDATE_TIMES = 10       # and how many times to update\n",
    "\n",
    "# For Cumulative Reward\n",
    "GAMMA = 0.99            # discount factor\n",
    "\n",
    "## For Q Network\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "OU_SIGMA = 0.2          # Ornstein-Uhlenbeck noise parameter\n",
    "OU_THETA = 0.15         # Ornstein-Uhlenbeck noise parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 33\n",
    "action_size = 4\n",
    "random_seed = 1\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network (w/ Target Network)\n",
    "actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "actor_optimizer = optim.Adam(actor_local.parameters(), lr=LR_ACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network (w/ Target Network)\n",
    "critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "critic_optimizer = optim.Adam(critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_targets(critic_local, critic_target)\n",
    "update_targets(actor_local, actor_target)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some util and data variables\n",
    "mu = 0.\n",
    "replay_memory = deque(maxlen=BUFFER_SIZE)\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "# Noise process\n",
    "noise = OUNoise(action_size, random_seed, mu, OU_SIGMA, OU_THETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#Run this cell in Udacity workspace. Otherwise run the cell below\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run this cell locally on mac only. Otherwise run the above cell.\n",
    "#Renamed the Reacher.app to Reacher20.app to keep it seperate with a single agent version\n",
    "\n",
    "#env = UnityEnvironment(file_name=\"Reacher20.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of our agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment (Optional Step)\n",
    "\n",
    "In the next code cell, we will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, we will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows us to observe the agent, as it moves through the environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.135499996971339\n",
      "Total steps in this episode: 1001\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "steps = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    steps += 1\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "print('Total steps in this episode: {}'.format(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent - one of the core logic of DDPG - To run INFERENCE on Actor Neural Network\n",
    "def agent(state, add_noise=True):\n",
    "    \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    actor_local.eval()\n",
    "    with torch.no_grad():\n",
    "        action = actor_local(state).cpu().data.numpy()\n",
    "    actor_local.train()\n",
    "    if add_noise:\n",
    "        action +=noise.sample()\n",
    "    return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Training of the Agent\n",
    "\n",
    "Or go straight to step 8 to test agent loaded with previuous trained weights (in its Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Agent - One of the core logic of DDPG - To TRAIN Actor and Critic Neural NETWORK\n",
    "def train_agent(state, action, reward, next_state, done, timestep):\n",
    "    \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "    Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "    where:\n",
    "        actor_target(state) -> action\n",
    "        critic_target(state, action) -> Q-value\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "       \n",
    "    e = experience(state, action, reward, next_state, done)\n",
    "    # Save experience / reward\n",
    "    replay_memory.append(e)\n",
    "        \n",
    "    # Learn, if enough samples are available in memory\n",
    "    if len(replay_memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "        for _ in range(UPDATE_TIMES):\n",
    "            \n",
    "            experiences = sample(replay_memory, BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            actions_next = actor_target(next_states)\n",
    "            Q_targets_next = critic_target(next_states, actions_next)\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))\n",
    "            # Compute critic loss\n",
    "            Q_expected = critic_local(states, actions)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            # Minimize the loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            U.clip_grad_norm_(critic_local.parameters(), 1)\n",
    "            critic_optimizer.step()\n",
    "\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = actor_local(states)\n",
    "            actor_loss = -critic_local(states, actions_pred).mean()\n",
    "            # Minimize the loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            update_targets(critic_local, critic_target, TAU)\n",
    "            update_targets(actor_local, actor_target, TAU) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Main Function - Define Training \n",
    "\n",
    "Or go straight to step 8 to test agent loaded with previuous trained weights (in its Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000, solved_score=30.0, consec_episodes=100, print_every=1, old_episodes=0, old_mov_avg=0.0):\n",
    "    \n",
    "    mean_scores = []                               # list of mean scores from each episode\n",
    "    min_scores = []                                # list of lowest scores from each episode\n",
    "    max_scores = []                                # list of highest scores from each episode\n",
    "    best_score = -np.inf\n",
    "    scores_deque = deque(maxlen=consec_episodes)   # mean scores from most recent episodes\n",
    "    moving_avgs = []                               # list of moving averages    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #reset the environment, noise and score\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations             # get current state for each agent\n",
    "        noise.reset()                                     # reset the noise\n",
    "        scores = np.zeros(num_agents)                     # initialize scores for each agent\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = agent(states, add_noise=True)       # select actions with exploration\n",
    "            env_info = env.step(actions)[brain_name]      # send actions to the environment\n",
    "            next_states = env_info.vector_observations    # get next states\n",
    "            rewards = env_info.rewards                    # get the rewards \n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            # save experience to replay buffer, perform learning step at defined interval\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):            \n",
    "                train_agent(state, action, reward, next_state, done, t) # train Actor and Crtic Neural Network\n",
    "            \n",
    "            states = next_states\n",
    "            \n",
    "            scores += rewards                             # add rewards to current scores \n",
    " \n",
    "            if np.any(dones):            # exit loop when episode ends\n",
    "                break     \n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        min_scores.append(np.min(scores))             # save lowest score for a single agent\n",
    "        max_scores.append(np.max(scores))             # save highest score for a single agent        \n",
    "        mean_scores.append(np.mean(scores))           # save mean score for the episode\n",
    "        scores_deque.append(mean_scores[-1])          # save mean score to deque\n",
    "        moving_avgs.append(np.mean(scores_deque))     # save moving average     \n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {} ({} sec)  -- \\tMin: {:.1f}\\tMax: {:.1f}\\tMean: {:.1f}\\tMov. Avg: {:.1f}'.format(\\\n",
    "                  i_episode, round(duration), min_scores[-1], max_scores[-1], mean_scores[-1], moving_avgs[-1])) \n",
    "            \n",
    "        if mean_scores[-1] > best_score:\n",
    "            torch.save(actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(critic_local.state_dict(), 'checkpoint_critic.pth')  \n",
    "         \n",
    "        real_episode = i_episode + old_episodes\n",
    "        real_moving_avgs = (moving_avgs[-1]*i_episode + old_mov_avg*old_episodes)/real_episode\n",
    "        \n",
    "        if i_episode % (print_every + 3) == 0:\n",
    "            print('\\rIncluding Previous Runs -- Total Episodes {}  -- \\tReal Mov. Avg: {:.1f}'.format(\\\n",
    "                  real_episode, real_moving_avgs)) \n",
    "        \n",
    "        if real_moving_avgs >= solved_score and real_episode >= consec_episodes:\n",
    "            print('\\nEnvironment SOLVED in {} episodes!\\tMoving Average ={:.1f} over last {} episodes'.format(\\\n",
    "                                    real_episode-consec_episodes, real_moving_avgs, consec_episodes))    \n",
    "            torch.save(actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "\n",
    "            \n",
    "    return mean_scores, moving_avgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Start Training (Optional) - 1st Run\n",
    "### (Skip to step 8 to test the environment with trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 (115 sec)  -- \tMin: 0.4\tMax: 5.0\tMean: 2.0\tMov. Avg: 2.0\n",
      "Episode 2 (115 sec)  -- \tMin: 0.3\tMax: 4.1\tMean: 2.3\tMov. Avg: 2.2\n",
      "Episode 3 (116 sec)  -- \tMin: 1.5\tMax: 5.2\tMean: 3.5\tMov. Avg: 2.6\n",
      "Episode 4 (118 sec)  -- \tMin: 2.0\tMax: 6.5\tMean: 4.1\tMov. Avg: 3.0\n",
      "Episode 5 (118 sec)  -- \tMin: 2.0\tMax: 5.7\tMean: 3.7\tMov. Avg: 3.1\n",
      "Episode 6 (119 sec)  -- \tMin: 2.4\tMax: 5.9\tMean: 4.2\tMov. Avg: 3.3\n",
      "Episode 7 (120 sec)  -- \tMin: 3.2\tMax: 13.0\tMean: 5.7\tMov. Avg: 3.7\n",
      "Episode 8 (121 sec)  -- \tMin: 5.1\tMax: 9.5\tMean: 7.3\tMov. Avg: 4.1\n",
      "Episode 9 (121 sec)  -- \tMin: 4.7\tMax: 11.4\tMean: 8.1\tMov. Avg: 4.6\n",
      "Episode 10 (119 sec)  -- \tMin: 4.1\tMax: 11.1\tMean: 8.2\tMov. Avg: 4.9\n",
      "Episode 11 (121 sec)  -- \tMin: 5.9\tMax: 13.1\tMean: 8.9\tMov. Avg: 5.3\n",
      "Episode 12 (122 sec)  -- \tMin: 6.6\tMax: 16.7\tMean: 11.0\tMov. Avg: 5.8\n",
      "Episode 13 (124 sec)  -- \tMin: 9.2\tMax: 14.5\tMean: 11.8\tMov. Avg: 6.2\n",
      "Episode 14 (126 sec)  -- \tMin: 8.3\tMax: 15.8\tMean: 12.9\tMov. Avg: 6.7\n",
      "Episode 15 (127 sec)  -- \tMin: 7.7\tMax: 15.6\tMean: 11.8\tMov. Avg: 7.0\n",
      "Episode 16 (130 sec)  -- \tMin: 7.5\tMax: 16.3\tMean: 12.0\tMov. Avg: 7.3\n",
      "Episode 17 (132 sec)  -- \tMin: 8.2\tMax: 15.6\tMean: 12.2\tMov. Avg: 7.6\n",
      "Episode 18 (134 sec)  -- \tMin: 8.9\tMax: 16.3\tMean: 13.4\tMov. Avg: 8.0\n",
      "Episode 19 (135 sec)  -- \tMin: 10.2\tMax: 19.8\tMean: 14.5\tMov. Avg: 8.3\n",
      "Episode 20 (139 sec)  -- \tMin: 10.3\tMax: 20.2\tMean: 15.1\tMov. Avg: 8.6\n",
      "Episode 21 (142 sec)  -- \tMin: 11.3\tMax: 19.7\tMean: 14.4\tMov. Avg: 8.9\n",
      "Episode 22 (144 sec)  -- \tMin: 11.1\tMax: 19.3\tMean: 16.1\tMov. Avg: 9.2\n",
      "Episode 23 (144 sec)  -- \tMin: 13.6\tMax: 17.9\tMean: 15.5\tMov. Avg: 9.5\n",
      "Episode 24 (148 sec)  -- \tMin: 12.7\tMax: 22.0\tMean: 16.6\tMov. Avg: 9.8\n",
      "Episode 25 (154 sec)  -- \tMin: 8.2\tMax: 20.4\tMean: 16.3\tMov. Avg: 10.1\n",
      "Episode 26 (156 sec)  -- \tMin: 12.9\tMax: 25.5\tMean: 17.0\tMov. Avg: 10.3\n",
      "Episode 27 (158 sec)  -- \tMin: 13.7\tMax: 23.7\tMean: 18.0\tMov. Avg: 10.6\n",
      "Episode 28 (159 sec)  -- \tMin: 12.5\tMax: 20.1\tMean: 18.2\tMov. Avg: 10.9\n",
      "Episode 29 (160 sec)  -- \tMin: 12.3\tMax: 23.5\tMean: 19.3\tMov. Avg: 11.2\n",
      "Episode 30 (165 sec)  -- \tMin: 15.5\tMax: 26.9\tMean: 20.1\tMov. Avg: 11.5\n",
      "Episode 31 (164 sec)  -- \tMin: 10.1\tMax: 30.2\tMean: 18.9\tMov. Avg: 11.7\n",
      "Episode 32 (165 sec)  -- \tMin: 17.1\tMax: 27.0\tMean: 21.3\tMov. Avg: 12.0\n",
      "Episode 33 (171 sec)  -- \tMin: 16.3\tMax: 25.7\tMean: 21.3\tMov. Avg: 12.3\n",
      "Episode 34 (175 sec)  -- \tMin: 15.5\tMax: 28.1\tMean: 20.8\tMov. Avg: 12.5\n",
      "Episode 35 (173 sec)  -- \tMin: 16.8\tMax: 25.3\tMean: 21.6\tMov. Avg: 12.8\n",
      "Episode 36 (175 sec)  -- \tMin: 19.9\tMax: 24.8\tMean: 22.0\tMov. Avg: 13.1\n",
      "Episode 37 (177 sec)  -- \tMin: 19.6\tMax: 27.3\tMean: 22.4\tMov. Avg: 13.3\n",
      "Episode 38 (179 sec)  -- \tMin: 16.4\tMax: 27.3\tMean: 22.2\tMov. Avg: 13.5\n",
      "Episode 39 (182 sec)  -- \tMin: 18.5\tMax: 24.1\tMean: 21.5\tMov. Avg: 13.7\n",
      "Episode 40 (184 sec)  -- \tMin: 19.3\tMax: 25.9\tMean: 22.2\tMov. Avg: 14.0\n",
      "Episode 41 (189 sec)  -- \tMin: 15.5\tMax: 26.1\tMean: 21.1\tMov. Avg: 14.1\n",
      "Episode 42 (195 sec)  -- \tMin: 15.5\tMax: 28.9\tMean: 20.7\tMov. Avg: 14.3\n",
      "Episode 43 (193 sec)  -- \tMin: 17.2\tMax: 25.7\tMean: 22.2\tMov. Avg: 14.5\n",
      "Episode 44 (199 sec)  -- \tMin: 16.7\tMax: 29.3\tMean: 21.8\tMov. Avg: 14.6\n",
      "Episode 45 (203 sec)  -- \tMin: 18.3\tMax: 29.3\tMean: 23.0\tMov. Avg: 14.8\n",
      "Episode 46 (206 sec)  -- \tMin: 17.2\tMax: 27.5\tMean: 22.8\tMov. Avg: 15.0\n",
      "Episode 47 (209 sec)  -- \tMin: 17.0\tMax: 26.0\tMean: 21.0\tMov. Avg: 15.1\n",
      "Episode 48 (208 sec)  -- \tMin: 18.9\tMax: 27.2\tMean: 21.6\tMov. Avg: 15.3\n",
      "Episode 49 (205 sec)  -- \tMin: 17.9\tMax: 30.1\tMean: 23.2\tMov. Avg: 15.4\n",
      "Episode 50 (212 sec)  -- \tMin: 18.7\tMax: 31.3\tMean: 22.9\tMov. Avg: 15.6\n",
      "Episode 51 (210 sec)  -- \tMin: 17.2\tMax: 33.4\tMean: 22.7\tMov. Avg: 15.7\n",
      "Episode 52 (212 sec)  -- \tMin: 18.1\tMax: 25.8\tMean: 22.4\tMov. Avg: 15.8\n",
      "Episode 53 (214 sec)  -- \tMin: 20.1\tMax: 29.5\tMean: 23.7\tMov. Avg: 16.0\n",
      "Episode 54 (215 sec)  -- \tMin: 21.2\tMax: 29.5\tMean: 25.6\tMov. Avg: 16.2\n",
      "Episode 55 (215 sec)  -- \tMin: 23.2\tMax: 38.2\tMean: 28.9\tMov. Avg: 16.4\n",
      "Episode 56 (213 sec)  -- \tMin: 22.0\tMax: 31.1\tMean: 26.2\tMov. Avg: 16.6\n",
      "Episode 57 (210 sec)  -- \tMin: 21.4\tMax: 30.1\tMean: 27.5\tMov. Avg: 16.8\n",
      "Episode 58 (212 sec)  -- \tMin: 21.6\tMax: 32.6\tMean: 26.8\tMov. Avg: 16.9\n",
      "Episode 59 (214 sec)  -- \tMin: 26.6\tMax: 34.7\tMean: 30.2\tMov. Avg: 17.2\n",
      "Episode 60 (215 sec)  -- \tMin: 23.5\tMax: 33.7\tMean: 29.0\tMov. Avg: 17.4\n",
      "Episode 61 (216 sec)  -- \tMin: 21.8\tMax: 32.2\tMean: 28.0\tMov. Avg: 17.5\n",
      "Episode 62 (211 sec)  -- \tMin: 22.7\tMax: 33.7\tMean: 28.6\tMov. Avg: 17.7\n",
      "Episode 63 (215 sec)  -- \tMin: 21.1\tMax: 34.3\tMean: 28.9\tMov. Avg: 17.9\n",
      "Episode 64 (214 sec)  -- \tMin: 26.1\tMax: 35.7\tMean: 29.7\tMov. Avg: 18.1\n",
      "Episode 65 (214 sec)  -- \tMin: 26.9\tMax: 34.1\tMean: 31.1\tMov. Avg: 18.3\n",
      "Episode 66 (209 sec)  -- \tMin: 28.0\tMax: 35.3\tMean: 32.7\tMov. Avg: 18.5\n",
      "Episode 67 (213 sec)  -- \tMin: 22.6\tMax: 35.7\tMean: 30.6\tMov. Avg: 18.7\n",
      "Episode 68 (215 sec)  -- \tMin: 11.3\tMax: 35.9\tMean: 28.2\tMov. Avg: 18.8\n",
      "Episode 69 (211 sec)  -- \tMin: 24.3\tMax: 33.4\tMean: 29.8\tMov. Avg: 19.0\n",
      "Episode 70 (215 sec)  -- \tMin: 23.3\tMax: 34.6\tMean: 30.8\tMov. Avg: 19.1\n",
      "Episode 71 (215 sec)  -- \tMin: 21.3\tMax: 36.5\tMean: 32.6\tMov. Avg: 19.3\n",
      "Episode 72 (215 sec)  -- \tMin: 23.7\tMax: 34.6\tMean: 31.2\tMov. Avg: 19.5\n",
      "Episode 73 (216 sec)  -- \tMin: 26.0\tMax: 35.7\tMean: 33.1\tMov. Avg: 19.7\n",
      "Episode 74 (215 sec)  -- \tMin: 27.9\tMax: 36.4\tMean: 32.5\tMov. Avg: 19.9\n",
      "Episode 75 (215 sec)  -- \tMin: 26.8\tMax: 36.2\tMean: 30.9\tMov. Avg: 20.0\n",
      "Episode 76 (214 sec)  -- \tMin: 11.6\tMax: 39.0\tMean: 29.5\tMov. Avg: 20.1\n",
      "Episode 77 (215 sec)  -- \tMin: 20.5\tMax: 39.3\tMean: 31.7\tMov. Avg: 20.3\n"
     ]
    }
   ],
   "source": [
    "# run the training loop\n",
    "scores, avgs = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Continue Training (Optional) - 2nd Run\n",
    "### (Skip to step 8 to test the environment with trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 (111 sec)  -- \tMin: 10.3\tMax: 29.7\tMean: 16.4\tMov. Avg: 16.4\n",
      "Episode 2 (115 sec)  -- \tMin: 10.0\tMax: 25.3\tMean: 19.9\tMov. Avg: 18.1\n",
      "Episode 3 (115 sec)  -- \tMin: 15.4\tMax: 32.9\tMean: 26.9\tMov. Avg: 21.1\n",
      "Episode 4 (117 sec)  -- \tMin: 22.1\tMax: 37.2\tMean: 31.5\tMov. Avg: 23.7\n",
      "Episode 5 (117 sec)  -- \tMin: 29.6\tMax: 39.3\tMean: 34.8\tMov. Avg: 25.9\n",
      "Episode 7 (119 sec)  -- \tMin: 31.8\tMax: 39.0\tMean: 35.7\tMov. Avg: 28.6\n",
      "Episode 8 (120 sec)  -- \tMin: 29.8\tMax: 39.5\tMean: 35.2\tMov. Avg: 29.4\n",
      "Episode 9 (121 sec)  -- \tMin: 33.9\tMax: 38.6\tMean: 37.1\tMov. Avg: 30.3\n",
      "Episode 10 (121 sec)  -- \tMin: 33.5\tMax: 39.2\tMean: 37.5\tMov. Avg: 31.0\n",
      "Episode 11 (123 sec)  -- \tMin: 32.0\tMax: 38.9\tMean: 36.5\tMov. Avg: 31.5\n",
      "Episode 12 (124 sec)  -- \tMin: 32.4\tMax: 39.0\tMean: 36.2\tMov. Avg: 31.9\n",
      "Episode 13 (126 sec)  -- \tMin: 32.7\tMax: 39.4\tMean: 36.2\tMov. Avg: 32.2\n",
      "Episode 14 (127 sec)  -- \tMin: 34.7\tMax: 39.3\tMean: 37.7\tMov. Avg: 32.6\n",
      "Episode 15 (129 sec)  -- \tMin: 32.4\tMax: 39.5\tMean: 37.2\tMov. Avg: 32.9\n",
      "Episode 16 (131 sec)  -- \tMin: 31.2\tMax: 38.3\tMean: 35.8\tMov. Avg: 33.1\n",
      "Episode 17 (133 sec)  -- \tMin: 32.7\tMax: 39.1\tMean: 37.2\tMov. Avg: 33.4\n",
      "Episode 18 (135 sec)  -- \tMin: 32.3\tMax: 39.3\tMean: 37.7\tMov. Avg: 33.6\n",
      "Episode 19 (137 sec)  -- \tMin: 32.5\tMax: 39.5\tMean: 36.4\tMov. Avg: 33.7\n",
      "Episode 20 (139 sec)  -- \tMin: 30.0\tMax: 39.6\tMean: 36.6\tMov. Avg: 33.9\n",
      "Episode 21 (141 sec)  -- \tMin: 34.0\tMax: 39.4\tMean: 37.0\tMov. Avg: 34.0\n",
      "Episode 22 (143 sec)  -- \tMin: 32.9\tMax: 39.3\tMean: 37.6\tMov. Avg: 34.2\n",
      "Episode 23 (147 sec)  -- \tMin: 36.1\tMax: 39.3\tMean: 37.9\tMov. Avg: 34.4\n",
      "Episode 24 (149 sec)  -- \tMin: 32.8\tMax: 39.2\tMean: 37.7\tMov. Avg: 34.5\n",
      "Episode 25 (152 sec)  -- \tMin: 37.0\tMax: 39.6\tMean: 38.5\tMov. Avg: 34.7\n",
      "Episode 26 (153 sec)  -- \tMin: 34.3\tMax: 39.5\tMean: 38.0\tMov. Avg: 34.8\n",
      "Episode 27 (156 sec)  -- \tMin: 33.8\tMax: 39.0\tMean: 37.9\tMov. Avg: 34.9\n",
      "Episode 28 (159 sec)  -- \tMin: 35.5\tMax: 39.4\tMean: 37.9\tMov. Avg: 35.0\n",
      "Episode 29 (161 sec)  -- \tMin: 35.7\tMax: 39.5\tMean: 38.2\tMov. Avg: 35.1\n",
      "Episode 31 (166 sec)  -- \tMin: 35.0\tMax: 39.6\tMean: 37.8\tMov. Avg: 35.3\n",
      "Episode 32 (169 sec)  -- \tMin: 31.5\tMax: 38.5\tMean: 36.2\tMov. Avg: 35.3\n",
      "Episode 33 (172 sec)  -- \tMin: 35.8\tMax: 39.5\tMean: 38.3\tMov. Avg: 35.4\n",
      "Episode 34 (174 sec)  -- \tMin: 26.4\tMax: 39.4\tMean: 37.3\tMov. Avg: 35.4\n",
      "Episode 35 (177 sec)  -- \tMin: 36.3\tMax: 39.5\tMean: 38.0\tMov. Avg: 35.5\n",
      "Episode 36 (179 sec)  -- \tMin: 36.4\tMax: 39.5\tMean: 38.6\tMov. Avg: 35.6\n",
      "Episode 37 (182 sec)  -- \tMin: 35.5\tMax: 39.5\tMean: 37.7\tMov. Avg: 35.7\n",
      "Episode 38 (184 sec)  -- \tMin: 36.4\tMax: 39.5\tMean: 38.6\tMov. Avg: 35.7\n",
      "Episode 39 (186 sec)  -- \tMin: 34.4\tMax: 39.6\tMean: 37.9\tMov. Avg: 35.8\n",
      "Episode 40 (190 sec)  -- \tMin: 34.8\tMax: 39.3\tMean: 38.1\tMov. Avg: 35.9\n",
      "Episode 41 (193 sec)  -- \tMin: 31.2\tMax: 39.4\tMean: 36.6\tMov. Avg: 35.9\n",
      "Episode 42 (196 sec)  -- \tMin: 26.1\tMax: 38.6\tMean: 35.6\tMov. Avg: 35.9\n",
      "Episode 43 (199 sec)  -- \tMin: 34.8\tMax: 39.5\tMean: 38.2\tMov. Avg: 35.9\n",
      "Episode 44 (203 sec)  -- \tMin: 23.6\tMax: 39.6\tMean: 34.5\tMov. Avg: 35.9\n",
      "Episode 45 (203 sec)  -- \tMin: 33.5\tMax: 39.6\tMean: 37.5\tMov. Avg: 35.9\n"
     ]
    }
   ],
   "source": [
    "# run the training loop with the saved weights from the previous run\n",
    "actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "update_targets(critic_local, critic_target)\n",
    "update_targets(actor_local, actor_target)    \n",
    "scores, avgs = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Continue Training (Optional) - 3rd Run\n",
    "### (Skip to step 8 to test the environment with trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 (113 sec)  -- \tMin: 15.5\tMax: 34.3\tMean: 23.9\tMov. Avg: 23.9\n",
      "Episode 2 (116 sec)  -- \tMin: 17.0\tMax: 33.6\tMean: 26.1\tMov. Avg: 25.0\n",
      "Episode 3 (117 sec)  -- \tMin: 28.1\tMax: 35.0\tMean: 31.5\tMov. Avg: 27.2\n",
      "Episode 4 (118 sec)  -- \tMin: 24.7\tMax: 38.3\tMean: 31.7\tMov. Avg: 28.3\n",
      "Episode 6 (119 sec)  -- \tMin: 28.6\tMax: 38.2\tMean: 35.7\tMov. Avg: 30.6\n",
      "Episode 7 (121 sec)  -- \tMin: 26.0\tMax: 38.1\tMean: 34.7\tMov. Avg: 31.2\n",
      "Episode 8 (122 sec)  -- \tMin: 25.7\tMax: 37.6\tMean: 34.2\tMov. Avg: 31.6\n",
      "Episode 9 (123 sec)  -- \tMin: 28.9\tMax: 38.1\tMean: 34.2\tMov. Avg: 31.9\n",
      "Episode 10 (124 sec)  -- \tMin: 30.3\tMax: 38.5\tMean: 35.7\tMov. Avg: 32.2\n",
      "Episode 11 (125 sec)  -- \tMin: 22.9\tMax: 36.7\tMean: 31.7\tMov. Avg: 32.2\n",
      "Episode 12 (127 sec)  -- \tMin: 21.7\tMax: 39.3\tMean: 34.0\tMov. Avg: 32.3\n",
      "Episode 13 (128 sec)  -- \tMin: 20.1\tMax: 37.1\tMean: 32.0\tMov. Avg: 32.3\n",
      "Episode 14 (129 sec)  -- \tMin: 23.7\tMax: 39.4\tMean: 35.4\tMov. Avg: 32.5\n",
      "Episode 15 (131 sec)  -- \tMin: 29.0\tMax: 38.3\tMean: 35.6\tMov. Avg: 32.7\n",
      "Episode 16 (132 sec)  -- \tMin: 17.3\tMax: 38.6\tMean: 31.5\tMov. Avg: 32.7\n",
      "Episode 17 (136 sec)  -- \tMin: 31.7\tMax: 38.5\tMean: 35.3\tMov. Avg: 32.8\n",
      "Episode 18 (138 sec)  -- \tMin: 27.0\tMax: 38.0\tMean: 33.4\tMov. Avg: 32.8\n",
      "Episode 19 (141 sec)  -- \tMin: 31.9\tMax: 38.6\tMean: 35.1\tMov. Avg: 33.0\n",
      "Episode 20 (142 sec)  -- \tMin: 26.7\tMax: 37.5\tMean: 33.9\tMov. Avg: 33.0\n",
      "Episode 21 (145 sec)  -- \tMin: 20.5\tMax: 38.0\tMean: 31.1\tMov. Avg: 32.9\n",
      "Episode 22 (147 sec)  -- \tMin: 30.4\tMax: 37.8\tMean: 34.0\tMov. Avg: 33.0\n",
      "Episode 23 (149 sec)  -- \tMin: 26.8\tMax: 38.7\tMean: 34.1\tMov. Avg: 33.0\n",
      "Episode 24 (151 sec)  -- \tMin: 24.9\tMax: 36.6\tMean: 32.3\tMov. Avg: 33.0\n",
      "Episode 25 (154 sec)  -- \tMin: 24.1\tMax: 37.2\tMean: 31.6\tMov. Avg: 32.9\n",
      "Episode 26 (157 sec)  -- \tMin: 24.5\tMax: 38.0\tMean: 33.6\tMov. Avg: 33.0\n",
      "Episode 27 (159 sec)  -- \tMin: 26.2\tMax: 39.3\tMean: 32.9\tMov. Avg: 33.0\n",
      "Episode 28 (162 sec)  -- \tMin: 22.6\tMax: 38.6\tMean: 32.6\tMov. Avg: 32.9\n",
      "Episode 29 (165 sec)  -- \tMin: 17.3\tMax: 38.2\tMean: 32.8\tMov. Avg: 32.9\n",
      "Episode 30 (167 sec)  -- \tMin: 30.7\tMax: 39.4\tMean: 36.6\tMov. Avg: 33.1\n",
      "Episode 31 (170 sec)  -- \tMin: 32.7\tMax: 39.5\tMean: 36.2\tMov. Avg: 33.2\n",
      "Episode 32 (172 sec)  -- \tMin: 30.8\tMax: 39.0\tMean: 34.4\tMov. Avg: 33.2\n"
     ]
    }
   ],
   "source": [
    "# run the training loop with the saved weights from the previous run\n",
    "actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "update_targets(critic_local, critic_target)\n",
    "update_targets(actor_local, actor_target)    \n",
    "scores, avgs = ddpg(500, 1000, 30.0, 100, 1, 77+45, (77*20.3 + 45*35.9)/(77 + 45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Continue Training (Optional) - 4th Run\n",
    "### (Skip to step 8 to test the environment with trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 (115 sec)  -- \tMin: 12.7\tMax: 30.4\tMean: 20.8\tMov. Avg: 20.8\n",
      "Episode 2 (118 sec)  -- \tMin: 14.6\tMax: 34.4\tMean: 26.4\tMov. Avg: 23.6\n",
      "Episode 3 (119 sec)  -- \tMin: 17.0\tMax: 32.7\tMean: 25.4\tMov. Avg: 24.2\n",
      "Episode 4 (120 sec)  -- \tMin: 14.3\tMax: 37.3\tMean: 25.2\tMov. Avg: 24.4\n",
      "Including Previous Runs -- Total Episodes 158  -- \tReal Mov. Avg: 27.5\n",
      "Episode 5 (120 sec)  -- \tMin: 25.7\tMax: 38.5\tMean: 32.5\tMov. Avg: 26.0\n",
      "Episode 6 (122 sec)  -- \tMin: 21.4\tMax: 39.3\tMean: 33.6\tMov. Avg: 27.3\n",
      "Episode 7 (123 sec)  -- \tMin: 13.6\tMax: 38.6\tMean: 30.6\tMov. Avg: 27.8\n",
      "Episode 8 (123 sec)  -- \tMin: 21.2\tMax: 39.4\tMean: 32.7\tMov. Avg: 28.4\n",
      "Including Previous Runs -- Total Episodes 162  -- \tReal Mov. Avg: 27.6\n",
      "Episode 9 (125 sec)  -- \tMin: 22.0\tMax: 38.0\tMean: 32.2\tMov. Avg: 28.8\n",
      "Episode 10 (126 sec)  -- \tMin: 20.4\tMax: 37.8\tMean: 33.6\tMov. Avg: 29.3\n",
      "Episode 11 (127 sec)  -- \tMin: 19.0\tMax: 38.4\tMean: 34.6\tMov. Avg: 29.8\n",
      "Episode 12 (129 sec)  -- \tMin: 22.2\tMax: 39.1\tMean: 34.4\tMov. Avg: 30.2\n",
      "Including Previous Runs -- Total Episodes 166  -- \tReal Mov. Avg: 27.7\n",
      "Episode 13 (130 sec)  -- \tMin: 27.2\tMax: 38.5\tMean: 35.0\tMov. Avg: 30.5\n",
      "Episode 14 (132 sec)  -- \tMin: 28.5\tMax: 39.1\tMean: 34.8\tMov. Avg: 30.8\n",
      "Episode 15 (133 sec)  -- \tMin: 22.8\tMax: 37.4\tMean: 32.3\tMov. Avg: 30.9\n",
      "Episode 16 (135 sec)  -- \tMin: 5.5\tMax: 38.0\tMean: 32.8\tMov. Avg: 31.1\n",
      "Including Previous Runs -- Total Episodes 170  -- \tReal Mov. Avg: 27.9\n",
      "Episode 17 (137 sec)  -- \tMin: 19.2\tMax: 38.3\tMean: 35.7\tMov. Avg: 31.3\n",
      "Episode 18 (139 sec)  -- \tMin: 19.8\tMax: 38.2\tMean: 33.4\tMov. Avg: 31.4\n",
      "Episode 19 (141 sec)  -- \tMin: 8.1\tMax: 39.1\tMean: 31.4\tMov. Avg: 31.4\n",
      "Episode 20 (143 sec)  -- \tMin: 25.7\tMax: 38.8\tMean: 34.2\tMov. Avg: 31.6\n",
      "Including Previous Runs -- Total Episodes 174  -- \tReal Mov. Avg: 28.0\n",
      "Episode 21 (146 sec)  -- \tMin: 22.1\tMax: 39.2\tMean: 34.5\tMov. Avg: 31.7\n",
      "Episode 22 (148 sec)  -- \tMin: 24.6\tMax: 38.7\tMean: 32.4\tMov. Avg: 31.7\n",
      "Episode 23 (150 sec)  -- \tMin: 23.7\tMax: 38.8\tMean: 35.0\tMov. Avg: 31.9\n",
      "Episode 24 (153 sec)  -- \tMin: 34.7\tMax: 39.0\tMean: 37.1\tMov. Avg: 32.1\n",
      "Including Previous Runs -- Total Episodes 178  -- \tReal Mov. Avg: 28.2\n",
      "Episode 25 (155 sec)  -- \tMin: 28.7\tMax: 39.5\tMean: 36.5\tMov. Avg: 32.3\n",
      "Episode 26 (158 sec)  -- \tMin: 25.4\tMax: 39.2\tMean: 34.6\tMov. Avg: 32.4\n",
      "Episode 27 (160 sec)  -- \tMin: 29.9\tMax: 38.8\tMean: 35.0\tMov. Avg: 32.5\n",
      "Episode 28 (162 sec)  -- \tMin: 21.7\tMax: 38.5\tMean: 32.1\tMov. Avg: 32.5\n",
      "Including Previous Runs -- Total Episodes 182  -- \tReal Mov. Avg: 28.3\n",
      "Episode 29 (166 sec)  -- \tMin: 26.9\tMax: 38.0\tMean: 33.7\tMov. Avg: 32.5\n",
      "Episode 30 (168 sec)  -- \tMin: 23.2\tMax: 38.3\tMean: 34.1\tMov. Avg: 32.6\n",
      "Episode 31 (170 sec)  -- \tMin: 26.7\tMax: 38.2\tMean: 33.8\tMov. Avg: 32.6\n",
      "Episode 32 (172 sec)  -- \tMin: 29.7\tMax: 39.3\tMean: 34.6\tMov. Avg: 32.7\n",
      "Including Previous Runs -- Total Episodes 186  -- \tReal Mov. Avg: 28.4\n",
      "Episode 33 (175 sec)  -- \tMin: 27.5\tMax: 38.2\tMean: 34.8\tMov. Avg: 32.7\n",
      "Episode 34 (178 sec)  -- \tMin: 21.0\tMax: 39.4\tMean: 32.8\tMov. Avg: 32.7\n",
      "Episode 35 (180 sec)  -- \tMin: 27.6\tMax: 38.4\tMean: 33.6\tMov. Avg: 32.7\n",
      "Episode 36 (183 sec)  -- \tMin: 30.2\tMax: 38.7\tMean: 35.3\tMov. Avg: 32.8\n",
      "Including Previous Runs -- Total Episodes 190  -- \tReal Mov. Avg: 28.5\n",
      "Episode 37 (185 sec)  -- \tMin: 24.9\tMax: 39.6\tMean: 34.6\tMov. Avg: 32.9\n",
      "Episode 38 (188 sec)  -- \tMin: 31.1\tMax: 39.3\tMean: 36.8\tMov. Avg: 33.0\n",
      "Episode 39 (190 sec)  -- \tMin: 27.8\tMax: 37.5\tMean: 34.4\tMov. Avg: 33.0\n",
      "Episode 40 (193 sec)  -- \tMin: 26.9\tMax: 37.9\tMean: 33.3\tMov. Avg: 33.0\n",
      "Including Previous Runs -- Total Episodes 194  -- \tReal Mov. Avg: 28.7\n",
      "Episode 41 (196 sec)  -- \tMin: 16.5\tMax: 38.2\tMean: 33.0\tMov. Avg: 33.0\n",
      "Episode 42 (198 sec)  -- \tMin: 30.5\tMax: 39.0\tMean: 36.2\tMov. Avg: 33.1\n",
      "Episode 43 (201 sec)  -- \tMin: 32.1\tMax: 39.4\tMean: 35.9\tMov. Avg: 33.2\n",
      "Episode 44 (203 sec)  -- \tMin: 21.9\tMax: 39.6\tMean: 33.5\tMov. Avg: 33.2\n",
      "Including Previous Runs -- Total Episodes 198  -- \tReal Mov. Avg: 28.8\n",
      "Episode 45 (205 sec)  -- \tMin: 32.3\tMax: 39.6\tMean: 36.9\tMov. Avg: 33.2\n",
      "Episode 46 (208 sec)  -- \tMin: 32.6\tMax: 37.8\tMean: 35.6\tMov. Avg: 33.3\n",
      "Episode 47 (210 sec)  -- \tMin: 27.3\tMax: 39.4\tMean: 34.2\tMov. Avg: 33.3\n",
      "Episode 48 (212 sec)  -- \tMin: 30.9\tMax: 38.3\tMean: 35.1\tMov. Avg: 33.4\n",
      "Including Previous Runs -- Total Episodes 202  -- \tReal Mov. Avg: 28.9\n",
      "Episode 49 (215 sec)  -- \tMin: 31.3\tMax: 39.4\tMean: 36.2\tMov. Avg: 33.4\n",
      "Episode 50 (216 sec)  -- \tMin: 30.2\tMax: 39.0\tMean: 35.5\tMov. Avg: 33.5\n",
      "Episode 51 (218 sec)  -- \tMin: 30.8\tMax: 39.5\tMean: 35.4\tMov. Avg: 33.5\n",
      "Episode 52 (219 sec)  -- \tMin: 23.4\tMax: 38.8\tMean: 33.8\tMov. Avg: 33.5\n",
      "Including Previous Runs -- Total Episodes 206  -- \tReal Mov. Avg: 29.0\n",
      "Episode 53 (220 sec)  -- \tMin: 5.8\tMax: 39.0\tMean: 32.2\tMov. Avg: 33.5\n",
      "Episode 54 (219 sec)  -- \tMin: 22.4\tMax: 37.6\tMean: 32.4\tMov. Avg: 33.5\n",
      "Episode 55 (220 sec)  -- \tMin: 24.0\tMax: 39.2\tMean: 33.6\tMov. Avg: 33.5\n",
      "Episode 56 (220 sec)  -- \tMin: 11.8\tMax: 38.5\tMean: 24.4\tMov. Avg: 33.3\n",
      "Including Previous Runs -- Total Episodes 210  -- \tReal Mov. Avg: 29.1\n",
      "Episode 57 (220 sec)  -- \tMin: 15.1\tMax: 30.0\tMean: 24.5\tMov. Avg: 33.1\n",
      "Episode 58 (220 sec)  -- \tMin: 15.2\tMax: 38.6\tMean: 25.4\tMov. Avg: 33.0\n",
      "Episode 59 (220 sec)  -- \tMin: 20.4\tMax: 36.6\tMean: 29.1\tMov. Avg: 32.9\n",
      "Episode 60 (220 sec)  -- \tMin: 17.0\tMax: 34.1\tMean: 26.5\tMov. Avg: 32.8\n",
      "Including Previous Runs -- Total Episodes 214  -- \tReal Mov. Avg: 29.0\n",
      "Episode 61 (220 sec)  -- \tMin: 23.7\tMax: 34.6\tMean: 29.1\tMov. Avg: 32.8\n",
      "Episode 62 (219 sec)  -- \tMin: 11.0\tMax: 34.3\tMean: 28.4\tMov. Avg: 32.7\n",
      "Episode 63 (220 sec)  -- \tMin: 20.7\tMax: 39.5\tMean: 28.4\tMov. Avg: 32.6\n",
      "Episode 64 (219 sec)  -- \tMin: 22.0\tMax: 37.2\tMean: 30.2\tMov. Avg: 32.6\n",
      "Including Previous Runs -- Total Episodes 218  -- \tReal Mov. Avg: 29.0\n",
      "Episode 65 (219 sec)  -- \tMin: 11.2\tMax: 37.8\tMean: 32.9\tMov. Avg: 32.6\n",
      "Episode 66 (219 sec)  -- \tMin: 28.5\tMax: 37.6\tMean: 34.3\tMov. Avg: 32.6\n",
      "Episode 67 (219 sec)  -- \tMin: 25.8\tMax: 38.5\tMean: 35.5\tMov. Avg: 32.7\n",
      "Episode 68 (219 sec)  -- \tMin: 33.2\tMax: 38.2\tMean: 36.2\tMov. Avg: 32.7\n",
      "Including Previous Runs -- Total Episodes 222  -- \tReal Mov. Avg: 29.1\n",
      "Episode 69 (220 sec)  -- \tMin: 29.6\tMax: 39.4\tMean: 35.6\tMov. Avg: 32.8\n",
      "Episode 70 (220 sec)  -- \tMin: 29.5\tMax: 39.6\tMean: 34.6\tMov. Avg: 32.8\n",
      "Episode 71 (219 sec)  -- \tMin: 34.1\tMax: 39.5\tMean: 37.4\tMov. Avg: 32.9\n",
      "Episode 72 (221 sec)  -- \tMin: 34.3\tMax: 39.6\tMean: 37.8\tMov. Avg: 32.9\n",
      "Including Previous Runs -- Total Episodes 226  -- \tReal Mov. Avg: 29.3\n",
      "Episode 73 (220 sec)  -- \tMin: 31.0\tMax: 39.6\tMean: 37.8\tMov. Avg: 33.0\n",
      "Episode 74 (220 sec)  -- \tMin: 29.4\tMax: 39.1\tMean: 36.0\tMov. Avg: 33.0\n",
      "Episode 75 (220 sec)  -- \tMin: 12.4\tMax: 38.3\tMean: 32.6\tMov. Avg: 33.0\n",
      "Episode 76 (219 sec)  -- \tMin: 28.6\tMax: 37.2\tMean: 33.7\tMov. Avg: 33.0\n",
      "Including Previous Runs -- Total Episodes 230  -- \tReal Mov. Avg: 29.4\n",
      "Episode 77 (219 sec)  -- \tMin: 25.8\tMax: 39.3\tMean: 35.9\tMov. Avg: 33.1\n",
      "Episode 78 (219 sec)  -- \tMin: 27.2\tMax: 39.3\tMean: 34.7\tMov. Avg: 33.1\n",
      "Episode 79 (219 sec)  -- \tMin: 30.6\tMax: 39.4\tMean: 36.7\tMov. Avg: 33.1\n",
      "Episode 80 (219 sec)  -- \tMin: 35.6\tMax: 39.6\tMean: 38.1\tMov. Avg: 33.2\n",
      "Including Previous Runs -- Total Episodes 234  -- \tReal Mov. Avg: 29.5\n",
      "Episode 81 (219 sec)  -- \tMin: 36.0\tMax: 39.5\tMean: 38.5\tMov. Avg: 33.3\n",
      "Episode 82 (219 sec)  -- \tMin: 32.8\tMax: 39.5\tMean: 37.8\tMov. Avg: 33.3\n",
      "Episode 83 (219 sec)  -- \tMin: 34.5\tMax: 39.6\tMean: 37.5\tMov. Avg: 33.4\n",
      "Episode 84 (220 sec)  -- \tMin: 31.9\tMax: 39.4\tMean: 37.9\tMov. Avg: 33.4\n",
      "Including Previous Runs -- Total Episodes 238  -- \tReal Mov. Avg: 29.6\n",
      "Episode 85 (220 sec)  -- \tMin: 35.4\tMax: 39.5\tMean: 37.9\tMov. Avg: 33.5\n",
      "Episode 86 (220 sec)  -- \tMin: 35.0\tMax: 39.3\tMean: 37.2\tMov. Avg: 33.5\n",
      "Episode 87 (220 sec)  -- \tMin: 31.5\tMax: 39.2\tMean: 35.1\tMov. Avg: 33.5\n",
      "Episode 88 (221 sec)  -- \tMin: 31.6\tMax: 38.9\tMean: 35.4\tMov. Avg: 33.6\n",
      "Including Previous Runs -- Total Episodes 242  -- \tReal Mov. Avg: 29.7\n",
      "Episode 89 (220 sec)  -- \tMin: 18.5\tMax: 39.1\tMean: 34.6\tMov. Avg: 33.6\n",
      "Episode 90 (220 sec)  -- \tMin: 33.3\tMax: 39.3\tMean: 37.1\tMov. Avg: 33.6\n",
      "Episode 91 (221 sec)  -- \tMin: 20.7\tMax: 38.7\tMean: 35.9\tMov. Avg: 33.6\n",
      "Episode 92 (220 sec)  -- \tMin: 29.3\tMax: 39.2\tMean: 35.4\tMov. Avg: 33.7\n",
      "Including Previous Runs -- Total Episodes 246  -- \tReal Mov. Avg: 29.8\n",
      "Episode 93 (221 sec)  -- \tMin: 30.8\tMax: 39.4\tMean: 35.6\tMov. Avg: 33.7\n",
      "Episode 94 (220 sec)  -- \tMin: 30.8\tMax: 37.7\tMean: 35.2\tMov. Avg: 33.7\n",
      "Episode 95 (221 sec)  -- \tMin: 30.3\tMax: 36.4\tMean: 34.4\tMov. Avg: 33.7\n",
      "Episode 96 (220 sec)  -- \tMin: 31.6\tMax: 38.8\tMean: 34.9\tMov. Avg: 33.7\n",
      "Including Previous Runs -- Total Episodes 250  -- \tReal Mov. Avg: 29.9\n",
      "Episode 97 (220 sec)  -- \tMin: 30.0\tMax: 38.3\tMean: 35.1\tMov. Avg: 33.7\n",
      "Episode 98 (221 sec)  -- \tMin: 27.3\tMax: 38.4\tMean: 33.3\tMov. Avg: 33.7\n",
      "Episode 99 (221 sec)  -- \tMin: 8.1\tMax: 39.4\tMean: 33.1\tMov. Avg: 33.7\n",
      "Episode 100 (221 sec)  -- \tMin: 27.0\tMax: 39.4\tMean: 36.3\tMov. Avg: 33.7\n",
      "Including Previous Runs -- Total Episodes 254  -- \tReal Mov. Avg: 30.0\n",
      "Episode 101 (221 sec)  -- \tMin: 31.3\tMax: 39.0\tMean: 35.7\tMov. Avg: 33.9\n",
      "\n",
      "Environment SOLVED in 155 episodes!\tMoving Average =30.1 over last 100 episodes\n"
     ]
    }
   ],
   "source": [
    "# run the training loop with the saved weights from the previous run\n",
    "actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "update_targets(critic_local, critic_target)\n",
    "update_targets(actor_local, actor_target)    \n",
    "with active_session(): ### Remember to comment this line if running outside Udacity workspace\n",
    "    scores, avgs = ddpg(500, 1000, 30.0, 100, 1, 77+45+32, (77*20.3 + 45*35.9 + 32*33.2)/(77 + 45 + 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot the training run\n",
    "### (Skip to step 8 to test the environment with trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_from_all_runs = [\\\n",
    "2,\\\n",
    "2.3,\\\n",
    "3.5,\\\n",
    "4.1,\\\n",
    "3.7,\\\n",
    "4.2,\\\n",
    "5.7,\\\n",
    "7.3,\\\n",
    "8.1,\\\n",
    "8.2,\\\n",
    "8.9,\\\n",
    "11,\\\n",
    "11.8,\\\n",
    "12.9,\\\n",
    "11.8,\\\n",
    "12,\\\n",
    "12.2,\\\n",
    "13.4,\\\n",
    "14.5,\\\n",
    "15.1,\\\n",
    "14.4,\\\n",
    "16.1,\\\n",
    "15.5,\\\n",
    "16.6,\\\n",
    "16.3,\\\n",
    "17,\\\n",
    "18,\\\n",
    "18.2,\\\n",
    "19.3,\\\n",
    "20.1,\\\n",
    "18.9,\\\n",
    "21.3,\\\n",
    "21.3,\\\n",
    "20.8,\\\n",
    "21.6,\\\n",
    "22,\\\n",
    "22.4,\\\n",
    "22.2,\\\n",
    "21.5,\\\n",
    "22.2,\\\n",
    "21.1,\\\n",
    "20.7,\\\n",
    "22.2,\\\n",
    "21.8,\\\n",
    "23,\\\n",
    "22.8,\\\n",
    "21,\\\n",
    "21.6,\\\n",
    "23.2,\\\n",
    "22.9,\\\n",
    "22.7,\\\n",
    "22.4,\\\n",
    "23.7,\\\n",
    "25.6,\\\n",
    "28.9,\\\n",
    "26.2,\\\n",
    "27.5,\\\n",
    "26.8,\\\n",
    "30.2,\\\n",
    "29,\\\n",
    "28,\\\n",
    "28.6,\\\n",
    "28.9,\\\n",
    "29.7,\\\n",
    "31.1,\\\n",
    "32.7,\\\n",
    "30.6,\\\n",
    "28.2,\\\n",
    "29.8,\\\n",
    "30.8,\\\n",
    "32.6,\\\n",
    "31.2,\\\n",
    "33.1,\\\n",
    "32.5,\\\n",
    "30.9,\\\n",
    "29.5,\\\n",
    "31.7,\\\n",
    "16.4,\\\n",
    "19.9,\\\n",
    "26.9,\\\n",
    "31.5,\\\n",
    "34.8,\\\n",
    "35.7,\\\n",
    "35.2,\\\n",
    "37.1,\\\n",
    "37.5,\\\n",
    "36.5,\\\n",
    "36.2,\\\n",
    "36.2,\\\n",
    "37.7,\\\n",
    "37.2,\\\n",
    "35.8,\\\n",
    "37.2,\\\n",
    "37.7,\\\n",
    "36.4,\\\n",
    "36.6,\\\n",
    "37,\\\n",
    "37.6,\\\n",
    "37.9,\\\n",
    "37.7,\\\n",
    "38.5,\\\n",
    "38,\\\n",
    "37.9,\\\n",
    "37.9,\\\n",
    "38.2,\\\n",
    "37.8,\\\n",
    "36.2,\\\n",
    "38.3,\\\n",
    "37.3,\\\n",
    "38,\\\n",
    "38.6,\\\n",
    "37.7,\\\n",
    "38.6,\\\n",
    "37.9,\\\n",
    "38.1,\\\n",
    "36.6,\\\n",
    "35.6,\\\n",
    "38.2,\\\n",
    "34.5,\\\n",
    "37.5,\\\n",
    "23.9,\\\n",
    "26.1,\\\n",
    "31.5,\\\n",
    "31.7,\\\n",
    "35.7,\\\n",
    "34.7,\\\n",
    "34.2,\\\n",
    "34.2,\\\n",
    "35.7,\\\n",
    "31.7,\\\n",
    "34,\\\n",
    "32,\\\n",
    "35.4,\\\n",
    "35.6,\\\n",
    "31.5,\\\n",
    "35.3,\\\n",
    "33.4,\\\n",
    "35.1,\\\n",
    "33.9,\\\n",
    "31.1,\\\n",
    "34,\\\n",
    "34.1,\\\n",
    "32.3,\\\n",
    "31.6,\\\n",
    "33.6,\\\n",
    "32.9,\\\n",
    "32.6,\\\n",
    "32.8,\\\n",
    "36.6,\\\n",
    "36.2,\\\n",
    "34.4,\\\n",
    "20.8,\\\n",
    "26.4,\\\n",
    "25.4,\\\n",
    "25.2,\\\n",
    "32.5,\\\n",
    "33.6,\\\n",
    "30.6,\\\n",
    "32.7,\\\n",
    "32.2,\\\n",
    "33.6,\\\n",
    "34.6,\\\n",
    "34.4,\\\n",
    "35,\\\n",
    "34.8,\\\n",
    "32.3,\\\n",
    "32.8,\\\n",
    "35.7,\\\n",
    "33.4,\\\n",
    "31.4,\\\n",
    "34.2,\\\n",
    "34.5,\\\n",
    "32.4,\\\n",
    "35,\\\n",
    "37.1,\\\n",
    "36.5,\\\n",
    "34.6,\\\n",
    "35,\\\n",
    "32.1,\\\n",
    "33.7,\\\n",
    "34.1,\\\n",
    "33.8,\\\n",
    "34.6,\\\n",
    "34.8,\\\n",
    "32.8,\\\n",
    "33.6,\\\n",
    "35.3,\\\n",
    "34.6,\\\n",
    "36.8,\\\n",
    "34.4,\\\n",
    "33.3,\\\n",
    "33,\\\n",
    "36.2,\\\n",
    "35.9,\\\n",
    "33.5,\\\n",
    "36.9,\\\n",
    "35.6,\\\n",
    "34.2,\\\n",
    "35.1,\\\n",
    "36.2,\\\n",
    "35.5,\\\n",
    "35.4,\\\n",
    "33.8,\\\n",
    "32.2,\\\n",
    "32.4,\\\n",
    "33.6,\\\n",
    "24.4,\\\n",
    "24.5,\\\n",
    "25.4,\\\n",
    "29.1,\\\n",
    "26.5,\\\n",
    "29.1,\\\n",
    "28.4,\\\n",
    "28.4,\\\n",
    "30.2,\\\n",
    "32.9,\\\n",
    "34.3,\\\n",
    "35.5,\\\n",
    "36.2,\\\n",
    "35.6,\\\n",
    "34.6,\\\n",
    "37.4,\\\n",
    "37.8,\\\n",
    "37.8,\\\n",
    "36,\\\n",
    "32.6,\\\n",
    "33.7,\\\n",
    "35.9,\\\n",
    "34.7,\\\n",
    "36.7,\\\n",
    "38.1,\\\n",
    "38.5,\\\n",
    "37.8,\\\n",
    "37.5,\\\n",
    "37.9,\\\n",
    "37.9,\\\n",
    "37.2,\\\n",
    "35.1,\\\n",
    "35.4,\\\n",
    "34.6,\\\n",
    "37.1,\\\n",
    "35.9,\\\n",
    "35.4,\\\n",
    "35.6,\\\n",
    "35.2,\\\n",
    "34.4,\\\n",
    "34.9,\\\n",
    "35.1,\\\n",
    "33.3,\\\n",
    "33.1,\\\n",
    "36.3,\\\n",
    "35.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d0c69aac8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores from all the runs\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_from_all_runs)+1), scores_from_all_runs)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Test Trained Agent in the Environment\n",
    "\n",
    "Once this cell is executed, we can watch the agent's performance.  A window should pop up that allows us to observe the agent, as it moves through the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 35.21199921295047\n",
      "Total steps in this episode: 1001\n"
     ]
    }
   ],
   "source": [
    "#Load the previous trained weights\n",
    "#Need to turn on the GPU to even test it. Otherwise torch load results in error. \n",
    "actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "steps = 0\n",
    "while True:\n",
    "    actions = agent(states, add_noise=False) \n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    steps += 1\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "print('Total steps in this episode: {}'.format(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
